{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from imp import reload\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "from pathlib import Path\n",
    "base_dir = str(Path('..').resolve())\n",
    "mesh_dir = base_dir + '/data/meshes'\n",
    "ais_mesh_dir = mesh_dir + '/ais_meshes'\n",
    "sys.path.append(base_dir + '/src')\n",
    "import ais_pipeline_scripting_tools as utils\n",
    "from ais_synapse_utils import load_ais_synapse_data\n",
    "\n",
    "import tqdm.autonotebook as tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial, sparse, stats\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from meshparty import trimesh_io, trimesh_vtk\n",
    "\n",
    "import vtk\n",
    "import matplotlib as mpl\n",
    "plt = mpl.pyplot\n",
    "\n",
    "from analysisdatalink.datalink_ext import AnalysisDataLinkExt as AnalysisDataLink\n",
    "from annotationframeworkclient.infoservice import InfoServiceClient\n",
    "\n",
    "from statsmodels.stats import multitest\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=os.path.expanduser('~/.config/pinky100/.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paper_styles import *\n",
    "set_rc_params(mpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_version = 141\n",
    "ais_table = 'ais_bounds_v3'   # manual_ais for core, ais_bounds for all\n",
    "chc_table = \"is_chandelier_v4\" # consult materialization for the best version\n",
    "soma_table = os.getenv('SOMA_TABLE')\n",
    "dataset_name = os.getenv('DATASET_NAME')\n",
    "sql_database_uri_base = os.getenv('MATERIALIZATION_DATABASE_URI_BASE')\n",
    "mesh_cv_path = os.getenv('CV_SOURCE')\n",
    "mesh_endpoint = os.getenv('MESH_ENDPOINT')\n",
    "synapse_table = os.getenv('AUTOMATED_SYNAPSE_TABLE')\n",
    "voxel_resolution = np.array(os.getenv('VOXEL_RESOLUTION').split(',')).astype(int)\n",
    "\n",
    "ais_synapse_data, aggregated_ais_syn_df, oids_failed, zero_syn_oids = \\\n",
    "    load_ais_synapse_data(chc_table, ais_table, data_version, base_dir + '/data')\n",
    "print('\\n')\n",
    "if oids_failed is not None:\n",
    "    if len(oids_failed) > 0:\n",
    "        print('Warning: {} oids failed on AIS synapse computation'.format(len(oids_failed)))\n",
    "    if len(zero_syn_oids) > 0:\n",
    "        print('Warning: some oids had no AIS synapses')\n",
    "        \n",
    "plot_dir = base_dir + '/plots/v{}'.format(data_version)\n",
    "if not os.path.exists(plot_dir):\n",
    "    print('Making new plot directory...')\n",
    "    os.mkdir(plot_dir)\n",
    "    \n",
    "dl = AnalysisDataLink(dataset_name=dataset_name,\n",
    "                      sqlalchemy_database_uri=sql_database_uri_base,\n",
    "                      materialization_version=data_version,\n",
    "                      verbose=False)\n",
    "\n",
    "chc_df = dl.query_cell_ids(chc_table)\n",
    "chc_ids = np.unique(chc_df[chc_df.func_id==1].pt_root_id)\n",
    "\n",
    "ais_df = dl.query_cell_ids(ais_table)\n",
    "complete_ais_ids = np.unique(ais_df.pt_root_id)\n",
    "\n",
    "# Use for the moment... probably need to fix when we explicitly set ids to ignore.\n",
    "from analyzable_soma_ids import oids_to_ignore\n",
    "oids_ignore = oids_to_ignore(data_version)\n",
    "\n",
    "ais_id_to_analyze = set(complete_ais_ids.astype(int)).difference(zero_syn_oids).difference(oids_failed).difference(oids_ignore)\n",
    "complete_ais_ids = np.array(list(ais_id_to_analyze))\n",
    "analyzable_soma_ids = np.array(list(set(complete_ais_ids).difference(oids_ignore)))\n",
    "\n",
    "in_analysis_set = np.isin(aggregated_ais_syn_df['post_pt_root_id'], list(ais_id_to_analyze))\n",
    "aggregated_ais_syn_df = aggregated_ais_syn_df[in_analysis_set]\n",
    "\n",
    "sv_df = dl.query_cell_types('soma_valence')\n",
    "aggregated_ais_syn_df = aggregated_ais_syn_df.merge(sv_df[['pt_root_id', 'pt_position']].rename(columns={'pt_position':'soma_position'}),\n",
    "                                                    how='left', left_on='post_pt_root_id', right_on='pt_root_id').drop(columns=('pt_root_id'))\n",
    "\n",
    "soma_to_null = aggregated_ais_syn_df['soma_position'].map(lambda x: np.all(np.isnan(x)))\n",
    "for ii in np.flatnonzero(soma_to_null):\n",
    "    aggregated_ais_syn_df.at[ii, 'soma_position'] = np.full(3, np.nan)\n",
    "    \n",
    "aggregated_ais_syn_df['soma_x'] = np.vstack(aggregated_ais_syn_df['soma_position'].values)[:,0] * 4\n",
    "aggregated_ais_syn_df['soma_y'] = np.vstack(aggregated_ais_syn_df['soma_position'].values)[:,1] * 4\n",
    "aggregated_ais_syn_df['soma_z'] = np.vstack(aggregated_ais_syn_df['soma_position'].values)[:,2] * 40\n",
    "\n",
    "aggregated_ais_syn_df['soma_x_um'] = np.vstack(aggregated_ais_syn_df['soma_position'].values)[:,0] * 4 / 1000\n",
    "aggregated_ais_syn_df['soma_y_um'] = np.vstack(aggregated_ais_syn_df['soma_position'].values)[:,1] * 4 / 1000\n",
    "aggregated_ais_syn_df['soma_z_um'] = np.vstack(aggregated_ais_syn_df['soma_position'].values)[:,2] * 40 / 1000\n",
    "\n",
    "all_syn_filename = base_dir + '/data/in/all_synapses_onto_pycs_v{}.h5'.format(data_version)\n",
    "if os.path.exists(all_syn_filename):\n",
    "    all_post_synapses_df = pd.read_hdf(all_syn_filename, 'all_post_synapses_df')\n",
    "else:\n",
    "    syn_dfs = []\n",
    "    for oid in tqdm.tqdm(np.unique(aggregated_ais_syn_df['post_pt_root_id'])):\n",
    "        syn_dfs.append(dl.query_synapses(synapse_table, post_ids=[oid]))\n",
    "    all_post_synapses_df = pd.concat(syn_dfs)\n",
    "    all_post_synapses_df.to_hdf(all_syn_filename, 'all_post_synapses_df')\n",
    "all_post_synapses_dec_df = all_post_synapses_df.merge(ais_synapse_data[['id', 'is_chandelier']], how='left', on='id')\n",
    "non_ais_post_syns = all_post_synapses_dec_df[all_post_synapses_dec_df['is_chandelier'].map(np.isnan)]\n",
    "\n",
    "full_soma_ids = dl.query_cell_ids('ais_analysis_soma')['pt_root_id'].values\n",
    "arbor_ais_df = aggregated_ais_syn_df[np.isin(aggregated_ais_syn_df['post_pt_root_id'], full_soma_ids)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_version = 126\n",
    "data_dir = base_dir + '/data/in'\n",
    "soma_filename = data_dir + '/soma_area_summary_v{}.csv'.format(original_data_version)\n",
    "\n",
    "if os.path.exists(soma_filename):\n",
    "    soma_df = pd.read_csv(soma_filename)\n",
    "else:\n",
    "    print('No soma data for this version!')\n",
    "\n",
    "if original_data_version != data_version:    \n",
    "    from analysisdatalink import version_map\n",
    "    soma_map_df = version_map.annotation_version_mapping('soma_valence', original_data_version, data_version, dataset_name, sql_database_uri_base)\n",
    "\n",
    "    soma_oid_map = {}\n",
    "    for _, row in soma_map_df.iterrows():\n",
    "        orig_oid = row['pt_root_id_v{}'.format(original_data_version)]\n",
    "        new_oid = row['pt_root_id_v{}'.format(data_version)]\n",
    "        soma_oid_map[orig_oid] = new_oid\n",
    "    soma_df['id'] = soma_df['id'].map(lambda x: soma_oid_map[x])\n",
    "    \n",
    "arbor_ais_df = arbor_ais_df.merge(soma_df, how='left', left_on='post_pt_root_id', right_on='id').drop(columns=['id', 'Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jitterbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial correlations of connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(6,6)\n",
    "\n",
    "y_offset = 160\n",
    "arbor_ais_df['soma_y_adj'] = arbor_ais_df['soma_y_um']-y_offset\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "sns.scatterplot(x='soma_x_um', y='soma_y_adj', size='num_cells_chc', data=arbor_ais_df, ax=ax, color='b')\n",
    "ax.set_aspect(1)\n",
    "ax.set_ylim((-5,125))\n",
    "ax.invert_yaxis()\n",
    "ax.legend().set_bbox_to_anchor([1,1])\n",
    "ax.grid(True, axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "sns.despine(offset=2, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Soma position x ($\\mu m$)')\n",
    "ax.set_xlabel('Soma position y ($\\mu m$)')\n",
    "\n",
    "fig.savefig(plot_dir + '/num_cells_chc_vs_soma_position_xy.pdf')\n",
    "\n",
    "figsize=(6,6)\n",
    "\n",
    "y_offset = 160\n",
    "arbor_ais_df['soma_y_adj'] = arbor_ais_df['soma_y_um']-y_offset\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "sns.scatterplot(x='soma_x_um', y='soma_y_adj', size='syn_mean_chc', data=arbor_ais_df, ax=ax, color='m')\n",
    "ax.set_aspect(1)\n",
    "ax.set_ylim((-5,125))\n",
    "ax.invert_yaxis()\n",
    "ax.legend().set_bbox_to_anchor([1,1])\n",
    "ax.grid(True, axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "sns.despine(offset=2, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Soma position x ($\\mu m$)')\n",
    "ax.set_xlabel('Soma position y ($\\mu m$)')\n",
    "\n",
    "fig.savefig(plot_dir + '/syn_mean_chc_vs_soma_position_xy.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(6,6)\n",
    "\n",
    "y_offset = 160\n",
    "arbor_ais_df['soma_y_adj'] = arbor_ais_df['soma_y_um']-y_offset\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "sns.scatterplot(x='soma_x_um', y='soma_y_adj', size='syn_net_chc', data=arbor_ais_df, ax=ax, color=chc_color)\n",
    "ax.set_aspect(1)\n",
    "ax.set_ylim((-5,125))\n",
    "ax.invert_yaxis()\n",
    "ax.legend().set_bbox_to_anchor([1,1])\n",
    "ax.grid(True, axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "sns.despine(offset=2, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Soma position x ($\\mu m$)')\n",
    "ax.set_xlabel('Soma position y ($\\mu m$)')\n",
    "\n",
    "fig.savefig(plot_dir + '/ais_input_vs_soma_position_xy.pdf', bbox_inches=\"tight\")\n",
    "\n",
    "###\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "sns.scatterplot(x='soma_x_um', y='soma_y_adj', size='syn_net_non', data=arbor_ais_df, ax=ax, color=non_color)\n",
    "ax.set_aspect(1)\n",
    "ax.set_ylim((-5,125))\n",
    "ax.invert_yaxis()\n",
    "ax.legend().set_bbox_to_anchor([1,1])\n",
    "ax.grid(True, axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "sns.despine(offset=2, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Soma position x ($\\mu m$)')\n",
    "ax.set_xlabel('Soma position y ($\\mu m$)')\n",
    "\n",
    "fig.savefig(plot_dir + '/ais_input_non_vs_soma_position_xy.pdf', bbox_inches=\"tight\")\n",
    "###\n",
    "\n",
    "figsize=(6,6)\n",
    "\n",
    "y_offset = 160\n",
    "arbor_ais_df['soma_y_adj'] = arbor_ais_df['soma_y_um']-y_offset\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "sns.scatterplot(x='soma_x_um', y='soma_y_adj', size='soma_synapses', data=arbor_ais_df, ax=ax, color='k')\n",
    "ax.set_aspect(1)\n",
    "ax.set_ylim((-5,125))\n",
    "ax.invert_yaxis()\n",
    "ax.legend().set_bbox_to_anchor([1,1])\n",
    "ax.grid(True, axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "sns.despine(offset=2, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Soma position x ($\\mu m$)')\n",
    "ax.set_xlabel('Soma position y ($\\mu m$)')\n",
    "\n",
    "fig.savefig(plot_dir + '/soma_input_vs_soma_position_xy.pdf', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meshparty import skeletonize\n",
    "from ossuary import synapse_flow\n",
    "\n",
    "def extract_axon_mesh(oid, dl, soma_radius, dataset_name=dataset_name):\n",
    "    soma_df = dl.query_cell_types('soma_valence_v2')\n",
    "    soma_loc = soma_df[soma_df['pt_root_id']==oid].values[0] * voxel_resolution\n",
    "    chc_mesh = mm.mesh(seg_id=oid)\n",
    "    chc_mesh.add_link_edges(int(oid), dataset_name)\n",
    "    chc_sk = skeletonize.skeletonize_mesh(chc_mesh,\n",
    "                                          soma_pt=soma_loc,\n",
    "                                          soma_radius=soma_radius)\n",
    "    pre_syn_df = dl.query_synapses(synapse_table, pre_ids=[oid])\n",
    "    post_syn_df = dl.query_synapses(synapse_table, post_ids=[oid])\n",
    "    \n",
    "    pre_pts = np.vstack(pre_syn_df[pre_syn_df['pre_pt_root_id']==oid]['ctr_pt_position']) * voxel_resolution\n",
    "    _, pre_inds = chc_mesh.kdtree.query(pre_pts)\n",
    "    pre_skinds = chc_sk.mesh_to_skel_map[pre_inds]\n",
    "\n",
    "    post_pts = np.vstack(post_syn_df[post_syn_df['post_pt_root_id']==oid]['ctr_pt_position']) * voxel_resolution\n",
    "    _, post_inds = chc_mesh.kdtree.query(post_pts)\n",
    "    post_skinds = chc_sk.mesh_to_skel_map[post_inds]\n",
    "\n",
    "    ax_split, qual = synapse_flow.find_axon_split(chc_sk, pre_skinds, post_skinds, return_quality=True, extend_to_segment=True)\n",
    "    return chc_mesh.apply_mask(np.isin(chc_sk.mesh_to_skel_map, chc_sk.downstream_nodes(ax_split)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = trimesh_io.MeshMeta(disk_cache_path=mesh_dir,\n",
    "                         cache_size=0, cv_path=mesh_cv_path)\n",
    "\n",
    "ais_meshes = []\n",
    "for oid in complete_ais_ids:\n",
    "    ais_file = ais_mesh_dir + '/{}_ais.h5'.format(oid)\n",
    "    if os.path.exists(ais_file):\n",
    "        ais_mesh = mm.mesh(filename=ais_file)\n",
    "        ais_meshes.append(ais_mesh)\n",
    "    else:\n",
    "        ais_meshes.append(None)\n",
    "        print('{} AIS not found!'.format(oid))\n",
    "\n",
    "in_df = dl.query_cell_types('interneurons_putative_ai', )\n",
    "chc_soma_ids = in_df[in_df['cell_type']=='chandelier']['pt_root_id'].values\n",
    "        \n",
    "chc_meshes = {}\n",
    "for oid in chc_ids:\n",
    "    if oid not in chc_soma_ids:\n",
    "        chc_mesh = mm.mesh(seg_id=oid)\n",
    "    else:\n",
    "        if os.path.exists(mesh_dir + '/{}_axon.h5'.format(oid)):\n",
    "            chc_mesh = mm.mesh(filename=mesh_dir + '/{}_axon.h5'.format(oid))\n",
    "        else:\n",
    "            chc_mesh = extract_axon_mesh(oid, dl, soma_radius=10000)\n",
    "            chc_mesh.write_to_file(mesh_dir + '/{}_axon.h5'.format(oid))\n",
    "    chc_meshes[oid] = chc_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Use the multi-utilities to get potential synapses from meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "from multiwrapper import multiprocessing_utils as mu\n",
    "def _multi_count_neighbors(args):\n",
    "    ais_kdt, chc_kdt, d_max = args\n",
    "    return ais_kdt.count_neighbors(chc_kdt, d_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_max = 10000\n",
    "multi_args = []\n",
    "for ais_ind, chc_oid in product(range(len(ais_meshes)), chc_meshes.keys()):\n",
    "    ais_mesh = ais_meshes[ais_ind]\n",
    "    chc_mesh = chc_meshes[chc_oid]\n",
    "    multi_args.append([ais_mesh.kdtree, chc_mesh.kdtree, d_max])\n",
    "    \n",
    "n_neib_list = mu.multiprocess_func(_multi_count_neighbors, multi_args, verbose=True)\n",
    "\n",
    "is_potential = defaultdict(dict)\n",
    "n_neib_list_copy = n_neib_list.copy()\n",
    "for ais_ind, chc_oid in product(range(len(ais_meshes)), chc_meshes.keys()):\n",
    "    is_potential[ais_ind][chc_oid] = n_neib_list_copy.pop(0)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pot_df = pd.DataFrame({'pyc_root_id': complete_ais_ids, 'num_potential':[sum(is_potential[ii].values()) for ii in range(len(complete_ais_ids))]})\n",
    "arbor_ais_df_pot = arbor_ais_df.merge(num_pot_df, left_on='post_pt_root_id', right_on='pyc_root_id', how='left').drop(columns=['pyc_root_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbor_ais_df_pot['conn_frac'] = arbor_ais_df_pot['num_cells_chc'] / arbor_ais_df_pot['num_potential'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "ax.hist(arbor_ais_df_pot['conn_frac'], bins=np.arange(0,1.01,0.05), edgecolor='w', color='k')\n",
    "ax.grid(True, axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "sns.despine(offset=2, trim=False, ax=ax)\n",
    "ax.set_xlabel('Connectivity fraction')\n",
    "ax.set_yticks(np.arange(0,21,5))\n",
    "ax.set_ylabel('# AISes')\n",
    "\n",
    "arbor_ais_df_pot['conn_frac'].describe()\n",
    "fig.savefig(fname=plot_dir+'/connectivity_fraction_histogram.pdf')\n",
    "\n",
    "conn_fract_described_df = arbor_ais_df_pot['conn_frac'].describe(percentiles=[])\n",
    "conn_fract_described_df.to_csv(plot_dir + '/connectivity_fraction_summary_v{}.csv'.format(data_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "y_offset = 160\n",
    "arbor_ais_df['soma_y_adj'] = arbor_ais_df['soma_y_um']-y_offset\n",
    "rel_ais = np.full(len(arbor_ais_df_pot), True)\n",
    "# rel_ais = arbor_ais_df_pot['conn_frac']<=0.5\n",
    "\n",
    "sns.scatterplot(x='conn_frac', y='soma_y_adj', data=arbor_ais_df_pot[rel_ais], ax=ax, color='k', s=30, alpha=0.8)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Connectivity fraction')\n",
    "ax.set_ylabel('Soma depth')\n",
    "\n",
    "ax.grid(True, axis='both')\n",
    "ax.set_axisbelow(True)\n",
    "sns.despine(offset=2, trim=False, ax=ax)\n",
    "fig.savefig(fname=plot_dir+'/conn_frac_vs_soma_y.pdf')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "sns.scatterplot(x='conn_frac', y='n_syn_soma', data=arbor_ais_df_pot[rel_ais], ax=ax, color='k', s=30, alpha=0.8)\n",
    "ax.set_xlabel('Connectivity fraction')\n",
    "ax.set_ylabel('# Syn Soma')\n",
    "\n",
    "ax.grid(True, axis='both')\n",
    "ax.set_axisbelow(True)\n",
    "sns.despine(offset=2, trim=False, ax=ax)\n",
    "fig.savefig(fname=plot_dir+'/conn_frac_vs_soma_synapses.pdf')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "sns.scatterplot(x='num_potential', y='soma_y_adj', data=arbor_ais_df_pot[rel_ais], ax=ax, color='k', s=30, alpha=0.8)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('# Potential')\n",
    "ax.set_ylabel('Soma depth')\n",
    "\n",
    "ax.grid(True, axis='both')\n",
    "ax.set_axisbelow(True)\n",
    "sns.despine(offset=2, trim=False, ax=ax)\n",
    "fig.savefig(fname=plot_dir+'/num_potential_vs_soma_y.pdf')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "sns.scatterplot(x='num_potential', y='n_syn_soma', data=arbor_ais_df_pot[rel_ais], ax=ax, color='k', s=30, alpha=0.8)\n",
    "ax.set_xlabel('# Potential')\n",
    "ax.set_ylabel('# Syn Soma')\n",
    "\n",
    "ax.grid(True, axis='both')\n",
    "ax.set_axisbelow(True)\n",
    "sns.despine(offset=2, trim=False, ax=ax)\n",
    "fig.savefig(fname=plot_dir+'/num_potential_vs_soma_synapses.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats import multitest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = [0.05, 0.01, 0.001]\n",
    "\n",
    "def assign_stars(pvals, star_ths):\n",
    "    n_stars = np.zeros(len(pvals))\n",
    "    for ii, star in enumerate(star_ths):\n",
    "        n_stars[pvals < star] = ii+1\n",
    "    return n_stars\n",
    "\n",
    "def plot_stars(xs, ys, n_stars, ax, xytext=(5,0), fontsize=12, fontweight=100, color=None, horizontalalignment='left'):\n",
    "    for x, y, ns in zip(xs, ys, n_stars):\n",
    "        if ns>0:\n",
    "            ax.annotate('*'*int(ns),\n",
    "                         (x,y),\n",
    "                         textcoords='offset points',\n",
    "                         xytext=xytext,\n",
    "                         fontsize=fontsize,\n",
    "                         fontweight=fontweight,\n",
    "                         color=color,\n",
    "                         horizontalalignment=horizontalalignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['n_syn_soma', 'soma_y_um', 'syn_net_non', 'soma_x_um']\n",
    "figsize=(4,3)\n",
    "\n",
    "xmin = 57000\n",
    "# xmin = -np.inf\n",
    "# rel_ais = (arbor_ais_df_pot['soma_x']>xmin*4)\n",
    "rel_ais = np.full(len(arbor_ais_df), True)\n",
    "\n",
    "X = arbor_ais_df_pot[rel_ais][columns].values.astype(float)\n",
    "y = arbor_ais_df_pot[rel_ais]['conn_frac'].values.astype(float)\n",
    "\n",
    "X[:,1] = -1*X[:,1]\n",
    "Xz = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "yz = (y-np.mean(y)) / np.std(y)\n",
    "\n",
    "mod = sm.OLS(yz, Xz)\n",
    "res_pot = mod.fit()\n",
    "# print(res_pot.summary())\n",
    "X = arbor_ais_df_pot[rel_ais][columns].values.astype(float)\n",
    "y = arbor_ais_df_pot[rel_ais]['num_potential'].values.astype(float)\n",
    "\n",
    "X[:,1] = -1*X[:,1]\n",
    "Xz = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "yz = (y-np.mean(y)) / np.std(y)\n",
    "\n",
    "mod = sm.OLS(yz, Xz)\n",
    "res_numpot = mod.fit()\n",
    "\n",
    "###\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "param_vals_pot = res_pot.params\n",
    "conf_ints_pot = res_pot.conf_int()\n",
    "xvals_pot = np.arange(len(param_vals_pot))\n",
    "for ii in np.arange(len(param_vals_pot)):\n",
    "    ax.plot([xvals_pot[ii], xvals_pot[ii]], conf_ints_pot[ii,:], linestyle='-', color='k')\n",
    "ax.plot(xvals_pot, param_vals_pot, marker='s', linestyle='', markeredgecolor='w', color='k')\n",
    "\n",
    "param_vals_numpot = res_numpot.params\n",
    "conf_ints_numpot = res_numpot.conf_int()\n",
    "xvals_numpot = np.arange(len(param_vals_numpot)) + 0.3\n",
    "for ii in np.arange(len(param_vals_numpot)):\n",
    "    ax.plot([xvals_numpot[ii], xvals_numpot[ii]], conf_ints_numpot[ii,:], linestyle='-', color='g')\n",
    "ax.plot(xvals_numpot, param_vals_numpot, marker='s', linestyle='', markeredgecolor='w', color='g')\n",
    "\n",
    "## Assign stars\n",
    "_, p_pot_coef_corr, _, _ = multitest.multipletests(res_pot.pvalues)\n",
    "n_star_corr = assign_stars(p_pot_coef_corr, stars)\n",
    "plot_stars(xvals_pot, conf_ints_pot[:,1], n_star_corr, ax, xytext=(0,0), color='k', horizontalalignment='center')\n",
    "\n",
    "_, p_numpot_coef_corr, _, _ = multitest.multipletests(res_numpot.pvalues)\n",
    "plot_stars(xvals_numpot, conf_ints_numpot[:,1], assign_stars(p_numpot_coef_corr, stars), ax, xytext=(0,0), color='k', horizontalalignment='center')\n",
    "\n",
    "ax.set_xticks(np.arange(len(param_vals_pot))+0.15)\n",
    "ax.set_ylim([-0.5, 0.9])\n",
    "ax.set_yticks(np.arange(-0.4,0.9,0.2))\n",
    "sns.despine(ax=ax, offset=5, trim=True)\n",
    "\n",
    "_=ax.set_xticklabels(['# Syn. soma', 'Soma pos. y', 'Syn. Non', 'Soma pos. x'], rotation=45)\n",
    "ax.plot([-0.5,len(param_vals_pot)-1+0.8],[0,0], 'k', zorder=1, linewidth=2)\n",
    "plt.grid(axis='y', which='major')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "line_chc = plt.Line2D([0],[0], linestyle='-', color='k')\n",
    "line_non = plt.Line2D([0],[0], linestyle='-', color='g')\n",
    "ax.legend([line_chc, line_non],['Conn. Frac.', '# Potential'])\n",
    "\n",
    "ax.set_ylabel('Standardized coefficient')\n",
    "ax.set_xlim([-0.15, len(param_vals_pot)+0.45-1])\n",
    "fig.savefig(plot_dir + '/potential_connectivity_standardized_coefficients.pdf')\n",
    "\n",
    "ols_res_df = pd.DataFrame({'Variable':['# Syn. soma', 'Soma pos. y', 'Syn. Non', 'Soma pos. x'],\n",
    "                           'ConnFrac corr':res_pot.params,\n",
    "                           'ConnFrac pval':p_pot_coef_corr,\n",
    "                           'NumPot corr':res_numpot.params,\n",
    "                           'NumPot pval':p_numpot_coef_corr,\n",
    "                           'ConnFrac R2': [res_pot.rsquared, np.nan, np.nan, np.nan],\n",
    "                           'NumPot R2': [res_numpot.rsquared, np.nan, np.nan, np.nan],\n",
    "                          })\n",
    "ols_res_df.to_csv(plot_dir + '/potential_connectivity_ols_fit.csv', index=False)\n",
    "ols_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Correct potential synapses on AIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svs = cv.CloudVolumeFactory(mesh_cv_path, mip=5, autocrop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(svs.bounds.to_list()).reshape(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mip=5\n",
    "cv_path = mesh_cv_path\n",
    "svs = cv.CloudVolumeFactory(cv_path, mip=mip)\n",
    "\n",
    "xmin = 1910\n",
    "xmax = 1930\n",
    "ymin = 1270\n",
    "ymax = 1290\n",
    "zmin = 2170\n",
    "zmax = 2180\n",
    "\n",
    "X, Y, Z = np.meshgrid(np.arange(xmin, xmax), np.arange(ymin,ymax), np.arange(zmin, zmax))\n",
    "\n",
    "xyzs = np.vstack([X.ravel(), Y.ravel(), Z.ravel()]).T * svs.resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_holder = np.zeros(X.shape)\n",
    "cv_bounds = np.array(svs.bounds.to_list()).reshape(2,3)\n",
    "xslice = slice(max((xmin, cv_bounds[0][0])), min((xmax, cv_bounds[1][0])), 1)\n",
    "yslice = slice(max((ymin, cv_bounds[0][1])), min((ymax, cv_bounds[1][1])), 1)\n",
    "zslice = slice(max((zmin, cv_bounds[0][2])), min((zmax, cv_bounds[1][2])), 1)\n",
    "sv_dat = np.squeeze(svs._cv[xslice, yslice, zslice])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice(max((ymin, cv_bounds[0][1])), min((ymax, cv_bounds[1][1])), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(sv_dat) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(xslice.start,xslice.stop)-xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(yslice.start,yslice.stop)-ymin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_holder[np.arange(xslice.start,xslice.stop)-xmin]\\\n",
    "         [:,np.arange(yslice.start,yslice.stop)-ymin]\\\n",
    "         [:,:,np.arange(zslice.start,zslice.stop)-zmin].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi, Yi, Zi = np.meshgrid(np.arange(xslice.start,xslice.stop)-xmin,\n",
    "                         np.arange(yslice.start,yslice.stop)-ymin,\n",
    "                         np.arange(zslice.start,zslice.stop)-zmin)\n",
    "sv_holder[Xi.ravel(), Yi.ravel(), Zi.ravel()] = sv_dat.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_holder[Xi.ravel(), Yi.ravel(), Zi.ravel()] = test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_holder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_holder[np.arange(xslice.start,xslice.stop)-xmin]\\\n",
    "         [:,np.arange(yslice.start,yslice.stop)-ymin]\\\n",
    "         [:,:,np.arange(zslice.start,zslice.stop)-zmin] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(xslice.start,xslice.stop)-xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudvolume as cv\n",
    "def segmentation_mesh_zeros(mesh, dmax_vec, cv_path, mip=5, cap_y=True):\n",
    "    svs = cv.CloudVolumeFactory(cv_path, mip=mip)\n",
    "    \n",
    "    print('\\tComputing grid coordinates...')\n",
    "    xmin, ymin, zmin = ((mesh.bounds[0] - max(dmax_vec)) / svs.resolution).astype(int)\n",
    "    xmax, ymax, zmax = ((mesh.bounds[1] + max(dmax_vec)) / svs.resolution).astype(int)\n",
    "    if cap_y:\n",
    "        ymin = (mesh.bounds[0][1] / svs.resolution[1]).astype(int)\n",
    "        ymax = (mesh.bounds[1][1] / svs.resolution[1]).astype(int)\n",
    "\n",
    "    X, Y, Z = np.meshgrid(np.arange(xmin, xmax), np.arange(ymin,ymax), np.arange(zmin, zmax), indexing='ij')\n",
    "\n",
    "    xyzs = np.vstack([X.ravel(), Y.ravel(), Z.ravel()]).T * svs.resolution\n",
    "    try:\n",
    "        dsp, _ = mesh.pykdtree.query(xyzs, distance_upper_bound=max(dmax_vec))\n",
    "    except:\n",
    "        dsp, _ = mesh.kdtree.query(xyzs, distance_upper_bound=max(dmax_vec))\n",
    "\n",
    "    print('\\tDownloading segmentation...')\n",
    "    sv_holder = np.zeros(X.shape)\n",
    "    cv_bounds = np.array(svs.bounds.to_list()).reshape(2,3)\n",
    "    xslice = slice(max((xmin, cv_bounds[0][0])), min((xmax, cv_bounds[1][0])), 1)\n",
    "    yslice = slice(max((ymin, cv_bounds[0][1])), min((ymax, cv_bounds[1][1])), 1)\n",
    "    zslice = slice(max((zmin, cv_bounds[0][2])), min((zmax, cv_bounds[1][2])), 1)\n",
    "    sv_dat = np.squeeze(svs._cv[xslice, yslice, zslice])\n",
    "    \n",
    "    Xi, Yi, Zi = np.meshgrid(np.arange(xslice.start,xslice.stop)-xmin,\n",
    "                             np.arange(yslice.start,yslice.stop)-ymin,\n",
    "                             np.arange(zslice.start,zslice.stop)-zmin,\n",
    "                             indexing='ij')\n",
    "    sv_holder[Xi.ravel(), Yi.ravel(), Zi.ravel()] = sv_dat.ravel()\n",
    "    \n",
    "    print('\\tComputing proximate grid volume...')\n",
    "    outside_seg = (sv_holder==0).astype(int).ravel()\n",
    "    frac_out = []\n",
    "    for dmax in dmax_vec:\n",
    "        valid=dsp<dmax\n",
    "        vox_outside = np.sum(outside_seg[valid])\n",
    "        vox_total = np.sum(valid)\n",
    "        frac_out.append(vox_outside / vox_total)\n",
    "    return frac_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "f = segmentation_mesh_zeros(ais_meshes[0], [5000, 7500, 10000], mesh_cv_path)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Z = np.meshgrid(np.arange(0,129), np.arange(0,300), np.arange(0,40), indexing='ij')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mip=5\n",
    "cv_path = mesh_cv_path\n",
    "dmax_vec = [5000, 7500, 10000]\n",
    "mesh = ais_meshes[0]\n",
    "cap_y = False\n",
    "\n",
    "svs = cv.CloudVolumeFactory(cv_path, mip=mip)\n",
    "\n",
    "print('\\tComputing grid coordinates...')\n",
    "xmin, ymin, zmin = ((mesh.bounds[0] - max(dmax_vec)) / svs.resolution).astype(int)\n",
    "xmax, ymax, zmax = ((mesh.bounds[1] + max(dmax_vec)) / svs.resolution).astype(int)\n",
    "if cap_y:\n",
    "    ymin = (mesh.bounds[0][1] / svs.resolution[1]).astype(int)\n",
    "    ymax = (mesh.bounds[1][1] / svs.resolution[1]).astype(int)\n",
    "\n",
    "X, Y, Z = np.meshgrid(np.arange(xmin, xmax), np.arange(ymin,ymax), np.arange(zmin, zmax), indexing='ij')\n",
    "\n",
    "xyzs = np.vstack([X.ravel(), Y.ravel(), Z.ravel()]).T * svs.resolutioæn\n",
    "try:\n",
    "    dsp, _ = mesh.pykdtree.query(xyzs, distance_upper_bound=max(dmax_vec))\n",
    "except:\n",
    "    dsp, _ = mesh.kdtree.query(xyzs, distance_upper_bound=max(ddmax_vec))\n",
    "\n",
    "print('\\tDownloading segmentation...')\n",
    "sv_holder = np.zeros(X.shape)\n",
    "cv_bounds = np.array(svs.bounds.to_list()).reshape(2,3)\n",
    "xslice = slice(max((xmin, cv_bounds[0][0])), min((xmax, cv_bounds[1][0])), 1)\n",
    "yslice = slice(max((ymin, cv_bounds[0][1])), min((ymax, cv_bounds[1][1])), 1)\n",
    "zslice = slice(max((zmin, cv_bounds[0][2])), min((zmax, cv_bounds[1][2])), 1)\n",
    "sv_dat = np.squeeze(svs._cv[xslice, yslice, zslice])\n",
    "\n",
    "Xi, Yi, Zi = np.meshgrid(np.arange(xslice.start,xslice.stop)-xmin,\n",
    "                         np.arange(yslice.start,yslice.stop)-ymin,\n",
    "                         np.arange(zslice.start,zslice.stop)-zmin, indexing='ij')\n",
    "sv_holder[Xi.ravel(), Yi.ravel(), Zi.ravel()] = sv_dat.ravel()\n",
    "\n",
    "print('\\tComputing proximate grid volume...')\n",
    "outside_seg = (sv_holder==0).astype(int).ravel()\n",
    "frac_out = []\n",
    "for dmax in dmax_vec:\n",
    "    valid=dsp<dmax\n",
    "    vox_outside = np.sum(outside_seg[valid])\n",
    "    vox_total = np.sum(valid)\n",
    "    frac_out.append(vox_outside / vox_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(xmin, xmax).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_dat = svs._cv[xslice, yslice, zslice]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmaxs = [5000, 7500, 10000]\n",
    "f_outs = []\n",
    "for mesh in tqdm.tqdm(ais_meshes):\n",
    "    f = segmentation_mesh_zeros(mesh, dmaxs, mesh_cv_path)\n",
    "    f_outs.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [5000, ]\n",
    "overlap_fs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_y = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mip = 5\n",
    "dmax = 10000\n",
    "svs = cv.CloudVolumeFactory(mesh_cv_path, mip=mip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = ais_meshes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmax = 10000\n",
    "xmin, ymin, zmin = ((mesh.bounds[0] - dmax) / svs.resolution).astype(int)\n",
    "xmax, ymax, zmax = ((mesh.bounds[1] + dmax) / svs.resolution).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_dat = svs._cv[xmin:xmax, ymin:ymax, zmin:zmax]\n",
    "outside_seg = sv_dat==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_dat = svs._cv[xmin:xmax, ymin:ymax, zmin:zmax]\n",
    "outside_seg = sv_dat==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_seg = np.squeeze(outside_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Z = np.meshgrid(np.arange(xmin, xmax), np.arange(ymin,ymax), np.arange(zmin, zmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsp, _ = mesh.pykdtree.query(xyzs, distance_upper_bound=dmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(~np.isinf(dsp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_seg_vec = (outside_seg==False).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_seg_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykdtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nils = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pykdtree.kdtree.KDTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X.ravel()[outside_seg_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_dat==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential connectivity\n",
    "\n",
    "Steps:\n",
    "1) Load all AIS meshes\n",
    "2) (Optional, but good idea) Clip beyond a certain lower distance\n",
    "3) Find which ChC axons come within distance d of each AIS\n",
    "4) Compute which fraction of these is actually connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = trimesh_io.MeshMeta(disk_cache_path=mesh_dir,\n",
    "                         cache_size=0, cv_path=mesh_cv_path)\n",
    "\n",
    "ais_meshes = []\n",
    "for oid in complete_ais_ids:\n",
    "    ais_file = ais_mesh_dir + '/{}_ais.h5'.format(oid)\n",
    "    if os.path.exists(ais_file):\n",
    "        ais_mesh = mm.mesh(filename=ais_file)\n",
    "        ais_meshes.append(ais_mesh)\n",
    "    else:\n",
    "        ais_meshes.append(None)\n",
    "        print('{} AIS not found!'.format(oid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chc_meshes = {}\n",
    "for oid in chc_ids:\n",
    "    chc_mesh = mm.mesh(seg_id=oid)\n",
    "    chc_meshes[oid] = chc_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = trimesh_vtk.mesh_actor(ais_meshes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimesh_vtk.render_actors([ma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykdtree import kdtree\n",
    "\n",
    "def is_potential(src_mesh, targ_mesh, max_d, cap_depth=True):\n",
    "    src_xyz = src_mesh.vertices\n",
    "    targ_xyz = targ_mesh.vertices\n",
    "    \n",
    "    if cap_depth is True:\n",
    "        max_y = np.max(targ_xyz[:,1])\n",
    "        min_y = np.min(targ_xyz[:,1])\n",
    "        within_bounds = (src_xyz[:,1]<max_y) & (src_xyz[:,1]>min_y)\n",
    "        if np.sum(within_bounds) == 0:\n",
    "            return False\n",
    "        else:\n",
    "            src_xyz = src_xyz[within_bounds]\n",
    "\n",
    "    ds, inds = kdtree.KDTree(targ_xyz).query(src_xyz)\n",
    "    return np.any(ds<max_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pot = []\n",
    "for ais_mesh in ais_meshes:\n",
    "    is_pot.append(is_potential(chc_meshes[10], ais_mesh, 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pot = []\n",
    "for ais_mesh in ais_meshes:\n",
    "    is_pot.append(is_potential(chc_meshes[10], ais_mesh, 10000))\n",
    "is_pot_df = pd.DataFrame({'pyc_root_id': complete_ais_ids, 'is_potential':is_pot})\n",
    "arbor_ais_df_temp=arbor_ais_df_temp.merge(is_pot_df, left_on='post_pt_root_id', right_on='pyc_root_id', how='left').drop(columns=['pyc_root_id'])['is_potential']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbor_ais_df_temp.merge(is_pot_df, left_on='post_pt_root_id', right_on='pyc_root_id', how='left').drop(columns=['pyc_root_id'])['is_potential']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“data_analysis”",
   "language": "python",
   "name": "jupyter_space"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
